<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
    <title>Project #2 - Video Processing</title>
    <style>
        html, body {
          margin: 0;       /* remove the default margin          */
          height: 100%;    /* make the html,body fill the page   */
        }
        canvas {
          display: block;  /* make the canvas act like a block   */
          width: 100%;     /* make the canvas fill its container */
          height: 100%;
        }
        #start {
          position: fixed;
          left: 0;
          top: 0;
          width: 100%;
          height: 100%;
          display: flex;
          justify-content: center;
          align-items: center;
        }
        #start>div {
          font-size: 200px;
          cursor: pointer;
        }
    </style>
  </head>
  <body>
    <canvas id="webgpu-canvas"></canvas>
    <pre id="fps">
    </pre>
    <div id="start">
      <div>▶️</div>
    </div>
  </body>
  <script type="importmap">
  {
      "imports": {
          "lil-gui": "https://cdn.jsdelivr.net/npm/lil-gui@0.19.2/dist/lil-gui.esm.js",
          "wgpu-matrix": "https://wgpu-matrix.org/dist/3.x/wgpu-matrix.module.js"
      }
  }
  </script>


  <script type="module">
import {mat4} from "wgpu-matrix";
import {GUI} from "lil-gui";

// bindings & locations can be assigned arbitrarily.
const binding_copy_tex_in = 2;
const binding_copy_tex_out = 4;
const binding_texquad_tex_out = 2;
const binding_texquad_sampler = 4;
const binding_texquad_uniform = 3;
const loc_position = 5;
const loc_inter_stage_uv = 1;

const workgroup_size = [8,8];

const FILTER_NONE = "no filter";

async function main() {

    const adapter = await navigator.gpu?.requestAdapter();
    const hasBGRA8unormStorage = adapter.features.has('bgra8unorm-storage');
    const device = await adapter?.requestDevice();
    if (!device) {
        fail('need a browser that supports WebGPU');
        return;
    }
    
    const canvas = document.querySelector('#webgpu-canvas');
    const context = canvas.getContext('webgpu');
    const presentationFormat = navigator.gpu.getPreferredCanvasFormat();
    context.configure({
        device,
        format: presentationFormat,
    });

    // create a typedarray to hold the a 4x4 f32 matrix uniform in JavaScript
    const videoMatrix = new Float32Array(16 * 4);
    // create a buffer for the uniform values
    const videoUniformBuffer = device.createBuffer({
        label: 'uniforms for video',
        size: videoMatrix.byteLength,
        usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST,
    });
    
    const renderPassDescriptor = {
        label: 'our basic canvas renderPass',
        colorAttachments: [
            {
                // view: <- to be filled out when we render
                clearValue: [0.3, 0.3, 0.3, 1],
                loadOp: 'clear',
                storeOp: 'store',
            },
        ],
    };
    
    function startPlayingAndWaitForVideo(video) {
        return new Promise((resolve, reject) => {
            video.addEventListener('error', reject);
            if ('requestVideoFrameCallback' in video) {
                video.requestVideoFrameCallback(resolve);
            } else {
                const timeWatcher = () => {
                    if (video.currentTime > 0) {
                        resolve();
                    } else {
                        requestAnimationFrame(timeWatcher);
                    }
                };
                timeWatcher();
            }
            video.play().catch(reject);
        });
    }
    
    function waitForClick() {
        return new Promise(resolve => {
            window.addEventListener(
                'click',
                () => {
                    document.querySelector('#start').style.display = 'none';
                    resolve();
                },
                { once: true });
        });
    }
    
    const video = document.createElement('video');
    video.muted = true;
    video.loop = true;
    video.preload = 'auto';
//    video.src = 'pexels-kosmo-politeska-5750980 (1080p).mp4'; 
    video.src = 'pexels-kosmo-politeska-5750980 (360p).mp4'; 
    await waitForClick();
    await startPlayingAndWaitForVideo(video);

    canvas.addEventListener('click', () => {
        if (video.paused) {
            video.play();
        } else {
            video.pause();
        }
    });

    let modules = {};

    // compute shader which simply copies the original video (stored in "textureIn")
    // to the target texture ("textureOut").
    // You should create similar shaders to apply various image filters.
    modules[FILTER_NONE] = device.createShaderModule({
        label: `${FILTER_NONE} shader`,
        code: `
            @group(0) @binding(${binding_copy_tex_in}) var textureIn: texture_external;
            @group(0) @binding(${binding_copy_tex_out}) var textureOut: texture_storage_2d<rgba8unorm, write>;
    
            @compute @workgroup_size(${workgroup_size[0]},${workgroup_size[1]}) fn main_comp(
                @builtin(global_invocation_id) id: vec3u
            ) {
                let uv = id.xy;

                var texel = textureLoad(textureIn, uv);
                textureStore(textureOut, uv, texel);
            }
        `,
    });

    modules["grayscale"] = device.createShaderModule({
        label: "grayscale shader",
        code: `
            @group(0) @binding(${binding_copy_tex_in}) var textureIn: texture_external;
            @group(0) @binding(${binding_copy_tex_out}) var textureOut: texture_storage_2d<rgba8unorm, write>;
    
            @compute @workgroup_size(${workgroup_size[0]},${workgroup_size[1]}) fn main_comp(
                @builtin(global_invocation_id) id: vec3u
            ) {
                let uv = id.xy;
    
                var texel = textureLoad(textureIn, uv);
                let grayscale = 0.48 * texel.r + 0.83 * texel.g + 0.24 * texel.b;
                textureStore(textureOut, uv, vec4f(grayscale, grayscale, grayscale, texel.a));
            }
        `,
    });


    modules["gaussian"] = device.createShaderModule({
        label: "Gaussian Blur shader",
        code: `
            // 텍스처와 출력 텍스처 바인딩
            @group(0) @binding(2) var textureIn: texture_external;
            @group(0) @binding(4) var textureOut: texture_storage_2d<rgba8unorm, write>;

            // 7x7 가우시안 필터 커널
            let Gaussian_filter_7x7: array<array<f32, 7>, 7> = array<array<f32, 7>, 7>(
                array<f32, 7>(0.00000067, 0.00002292, 0.00019117, 0.00038771, 0.00019117, 0.00002292, 0.00000067),
                array<f32, 7>(0.00002292, 0.00078633, 0.00655965, 0.01330373, 0.00655965, 0.00078633, 0.00002292),
                array<f32, 7>(0.00019117, 0.00665965, 0.05472157, 0.11098164, 0.05472157, 0.00655965, 0.00019117),
                array<f32, 7>(0.00038771, 0.01330373, 0.11098164, 0.22508352, 0.11098164, 0.01330373, 0.00038771),
                array<f32, 7>(0.00019117, 0.00665965, 0.05472157, 0.11098164, 0.05472157, 0.00655965, 0.00019117),
                array<f32, 7>(0.00002292, 0.00078633, 0.00655965, 0.01330373, 0.00655965, 0.00078633, 0.00002292),
                array<f32, 7>(0.00000067, 0.00002292, 0.00019117, 0.00038771, 0.00019117, 0.00002292, 0.00000067)
            );

            // 텍스처의 크기
            let textureSize: vec2<u32> = textureDimensions(textureIn).xy;

            // 범위 제한 함수 (in_range)
            fn in_range(value: u32, minValue: u32, maxValue: u32) -> u32 {
                return min(max(value, minValue), maxValue);
            }

            // 7x7 가우시안 필터 적용
            @compute @workgroup_size(8, 8) fn main_comp(@builtin(global_invocation_id) id: vec3u) {
                let uv = id.xy;

                // 필터링할 색상 값을 저장할 배열
                var color: vec3<f32> = vec3<f32>(0.0, 0.0, 0.0);

                // 7x7 커널을 사용하여 주변 픽셀을 필터링
                for (var i: i32 = -3; i <= 3; i = i + 1) {
                    for (var j: i32 = -3; j <= 3; j = j + 1) {
                        // 현재 픽셀의 주변 픽셀 위치 계산
                        let neighborUv: vec2<u32> = vec2<u32>(
                            in_range(uv.x + u32(i), 0u, textureSize.x - 1u),
                            in_range(uv.y + u32(j), 0u, textureSize.y - 1u)
                        );

                        // 주변 픽셀의 색상을 불러오고 가우시안 커널 값으로 곱함
                        let texel: vec4<f32> = textureLoad(textureIn, neighborUv);
                        let weight: f32 = Gaussian_filter_7x7[i + 3][j + 3]; // 커널 인덱스 계산

                        // 색상 값에 가중치를 곱해 더함
                        color = color + texel.rgb * weight;
                    }
                }

                // 결과를 출력 텍스처에 저장
                let offset = 4 * (uv.x + uv.y * textureSize.x);
                textureStore(textureOut, uv, vec4<f32>(color, 1.0)); // 알파 채널은 1.0으로 설정
            }
        `,
    });

    // The vertex & fragment shaders which draws a textured quad
    const moduleTexQuad = device.createShaderModule({
        label: 'textured quad shaders',
        code: `
            struct VertexInfo {
                @builtin(position) position: vec4f,
                @location(${loc_inter_stage_uv}) uv: vec2f
            };
            
            struct Uniforms {
                matrix: mat4x4f,
            };
            
            @group(0) @binding(${binding_texquad_uniform}) var<uniform> uni: Uniforms;
            
            @vertex fn vs(
                @location(${loc_position}) position: vec3f
            ) -> VertexInfo {
          
                var vertexOut: VertexInfo;
                vertexOut.position = uni.matrix * vec4f(position, 1.0);
                vertexOut.position = vec4f(vec3f(2,-2,0.0)*position, 1.0) + vec4f(-1,1,0,0);
                vertexOut.uv = position.xy;
                return vertexOut;
            }
            
            @group(0) @binding(${binding_texquad_sampler}) var ourSampler: sampler;
            @group(0) @binding(${binding_texquad_tex_out}) var ourTexture: texture_2d<f32>;
            
            @fragment fn fs(@location(${loc_inter_stage_uv}) uv: vec2f) -> @location(0) vec4f {
                return textureSampleBaseClampToEdge(
                    ourTexture,
                    ourSampler,
                    uv,
              );
            }
        `,
    });

    let pipelines = {};
    let filters = [];

    for(let filter in modules) {
        pipelines[filter] = device.createComputePipeline({
            label: `${filter} pipeline`,
            layout: "auto",
            compute: {
                module: modules[filter],
            },
        });
        filters.push(filter);
    }

    let menu = {filter:FILTER_NONE};

    const gui = new GUI();

    gui.add(menu, "filter", filters);
    
    const pipelineTexQuad = device.createRenderPipeline({
        label: 'textured quad pipeline',
        layout: 'auto',
        vertex: {
            module: moduleTexQuad,
            buffers: [
                {
                    arrayStride: 8,
                    attributes: [{
                        format: "float32x2",
                        offset: 0,
                        shaderLocation: loc_position,
                    }],
                }
            ],
        },
        fragment: {
            module: moduleTexQuad,
            targets: [{ format: presentationFormat }],
        },
    });
    
    const sampler = device.createSampler({
        magFilter: 'linear',
        minFilter: 'linear',
    });
 
    const textureOut = device.createTexture({
        label: "processed video texture",
        format: "rgba8unorm",
        size: [video.videoWidth, video.videoHeight],
        usage: GPUTextureUsage.TEXTURE_BINDING |
               GPUTextureUsage.STORAGE_BINDING,
    });

    const bindGroupTexQuad = device.createBindGroup({
        layout: pipelineTexQuad.getBindGroupLayout(0),
        entries: [
            { binding: binding_texquad_sampler, resource: sampler },
            { binding: binding_texquad_tex_out, resource: textureOut.createView() },
            { binding: binding_texquad_uniform, resource: { buffer: videoUniformBuffer }},
        ],
    });

    const num_workgroups = [Math.floor((video.videoWidth  + workgroup_size[0] - 1)/workgroup_size[0]),
                            Math.floor((video.videoHeight + workgroup_size[1] - 1)/workgroup_size[1])];


    // vertex attributes for two triangles of a quad
    const vertices = new Float32Array([
        // 1st triangle
        0, 0,
        1, 0,
        1, 1,
        // 2nd triangle
        0, 0,
        1, 1,
        0, 1
    ]);
    const vertexBuffer = device.createBuffer({
        label:"quad vertices",
        size: vertices.byteLength,
        usage: GPUBufferUsage.VERTEX | GPUBufferUsage.COPY_DST,
    });
    device.queue.writeBuffer(vertexBuffer, 0, vertices);


    function render() {

        const encoder = device.createCommandEncoder({ label: 'video processing encoder' });

        // 1st pass: image processing using a compute pipeline
        {
            const textureIn = device.importExternalTexture({source: video});
            
            // We have to create a bindgroup at every frame since we're using an "external texture."
            // See https://webgpufundamentals.org/webgpu/lessons/webgpu-textures-external-video.html
            const bindGroup = device.createBindGroup({
                layout: pipelines[menu.filter].getBindGroupLayout(0),
                entries: [
                    { binding: binding_copy_tex_in, resource: textureIn},
                    { binding: binding_copy_tex_out, resource: textureOut.createView()}, 
                ],
            });
            
            const pass = encoder.beginComputePass();
            
            pass.setPipeline(pipelines[menu.filter]);
            pass.setBindGroup(0, bindGroup);
            pass.dispatchWorkgroups(video.videoWidth, video.videoHeight);
            
            pass.end();
        }
        
        
        {
            const canvasTexture = context.getCurrentTexture().createView();
            renderPassDescriptor.colorAttachments[0].view = canvasTexture;

            const pass = encoder.beginRenderPass(renderPassDescriptor);
            
            // Create a matrix according to th current aspct ratio of the canvas
            // to keep the aspect ratio of the video.
            const canvasAspect = canvas.clientWidth / canvas.clientHeight;
            const videoAspect = video.videoWidth / video.videoHeight;
            const scale = canvasAspect > videoAspect
               ? [1, canvasAspect / videoAspect, 1]
               : [videoAspect / canvasAspect, 1, 1];
            
            const matrix = mat4.identity(videoMatrix);
            mat4.scale(matrix, scale, matrix);
            mat4.translate(matrix, [-1, 1, 0], matrix);
            mat4.scale(matrix, [2, -2, 1], matrix);
            
            device.queue.writeBuffer(videoUniformBuffer, 0, videoMatrix);
            
            pass.setPipeline(pipelineTexQuad);
            pass.setBindGroup(0, bindGroupTexQuad);
            pass.setVertexBuffer(0, vertexBuffer);
            pass.draw(6);  // call our vertex shader 6 times
            pass.end();
        }
       
        const commandBuffer = encoder.finish();
        device.queue.submit([commandBuffer]);
        
        requestAnimationFrame(render);
    }

    requestAnimationFrame(render);

    const observer = new ResizeObserver(entries => {
        for (const entry of entries) {
            const canvas = entry.target;
            const width = entry.contentBoxSize[0].inlineSize;
            const height = entry.contentBoxSize[0].blockSize;
            canvas.width = Math.max(1, Math.min(width, device.limits.maxTextureDimension2D));
            canvas.height = Math.max(1, Math.min(height, device.limits.maxTextureDimension2D));
        }
    });
    observer.observe(canvas);
}

function fail(msg) {
    alert(msg);
}

main();
  </script>
</html>
